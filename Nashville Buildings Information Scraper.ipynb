{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parasgulati8/Web-Scraper/blob/master/Nashville%20Buildings%20Information%20Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3g0wXYgZWnj",
        "colab_type": "code",
        "outputId": "135e6c68-53a2-4580-e623-9d351f15d14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgrx0afmY3hZ",
        "colab_type": "code",
        "outputId": "340a1d29-9cdb-48b8-ceb4-e50070786acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/'My Drive'/'Colab Notebooks'/dataset/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSDQMc2zmTMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from bs4 import BeautifulSoup as bs\n",
        "# import urllib3\n",
        "import requests\n",
        "# import json\n",
        "# from urllib.request import urlopen, urlretrieve, quote\n",
        "# from openpyxl import Workbook\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMSsqemjhkig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Property Summary Page\n",
        "def get_property_summary(index):\n",
        "    print('fetching Property Summary for ', index)\n",
        "    url = 'http://www.padctn.org/prc/property/' + str(index)\n",
        "    page = requests.get(url)\n",
        "    soup = bs(page.text, 'html.parser')\n",
        "    lists = soup.find_all('ul', attrs={'class':'att'})\n",
        "    table = dict()\n",
        "    for i in range(len(lists)):\n",
        "      lists[i] = lists[i].find_all('li')\n",
        "      lists[i] = [item.text for item in lists[i]]\n",
        "      \n",
        "      for j in range(len(lists[i])):\n",
        "        colon = lists[i][j].find(':')\n",
        "        lists[i][j] = [lists[i][j][:colon], lists[i][j][colon+2:]]\n",
        "          \n",
        "      for pair in lists[i]:\n",
        "        key , value = pair\n",
        "        table[key] = value\n",
        "    table['Property#'] = index\n",
        "    summary = pd.DataFrame(table, index=[index])\n",
        "    return summary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5xKfU764or-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Historical data\n",
        "def get_historical_data(index):\n",
        "    print('fetching historical data for ', index)\n",
        "    url = 'http://www.padctn.org/prc/property/' + str(index) + '/card/1/historical'\n",
        "    page = requests.get(url)\n",
        "    soup = bs(page.text, 'html.parser')\n",
        "    parcel = soup.h2.text\n",
        "    parcel = ' '.join(parcel.split()[6:])\n",
        "    dfs = pd.read_html(requests.get(url,  headers={'User-agent': 'Mozilla/5.0'}).text,\n",
        "                          attrs={\"class\":\"table table-condensed table-bordered\"})\n",
        "    sales_history, previous_appraisals = dfs[0], dfs[1]\n",
        "    sales_history['Map & Parcel'] = [parcel]*len(sales_history)\n",
        "    sales_history['Property #'] = [index]*len(sales_history)\n",
        "    previous_appraisals['Map & Parcel'] = [parcel]*len(previous_appraisals)\n",
        "    previous_appraisals['Property #'] = [index]*len(previous_appraisals)\n",
        "    return sales_history, previous_appraisals\n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9gpFrTdziWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building and Improvement Details Page\n",
        "def get_building_improvement(index):\n",
        "    print('fetching building improvement for ', index)\n",
        "    url = 'http://www.padctn.org/prc/property/' + str(index) + '/card/1/interior'\n",
        "    page = requests.get(url)\n",
        "    soup = bs(page.text, 'html.parser')\n",
        "    print('heading =',soup.h3.text)\n",
        "    parcel = ''.join(re.findall(r'\\d+.' , ' '.join((soup.h3.text).split())))[:-1] \n",
        "    # print('parcel = ', parcel)\n",
        "    lists = (soup.find_all('ul', attrs={'class':'att'}))\n",
        "    table = dict()\n",
        "    for i in range(len(lists)):\n",
        "      lists[i] = lists[i].find_all('li')\n",
        "      lists[i] = [item.text for item in lists[i]]\n",
        "      # lists[i] = [lists[i][j].split(': ') for j in range(len(lists[i]))]\n",
        "      for j in range(len(lists[i])):\n",
        "        colon = lists[i][j].find(':')\n",
        "        lists[i][j] = [lists[i][j][:colon], lists[i][j][colon+2:]]\n",
        "\n",
        "      for pair in lists[i]:\n",
        "        key , value = pair\n",
        "        table[key] = value\n",
        "    \n",
        "    temp_table = pd.DataFrame(table, index=[0])\n",
        "    temp_table['Property#'] = [index]\n",
        "    dfs = pd.read_html(requests.get(url,  headers={'User-agent': 'Mozilla/5.0'}).text)\n",
        "    building_footage, building_features = dfs[0], dfs[1]\n",
        "    building_features['Property#'] = [index]*(1 if len(building_features) == 0 else len(building_features))\n",
        "    building_footage['Property#'] = [index]*len(building_footage)\n",
        "    merged = pd.merge(building_footage, building_features, on='Property#')\n",
        "    building_improvement = pd.merge(merged, temp_table, on='Property#')\n",
        "    building_improvement['Map & Parcel'] = parcel\n",
        "    return building_improvement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgNWvoZzAxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def terminate(index, patience):\n",
        "    for i in range(index+1,index+patience):\n",
        "        temp_history, temp_appraisal = get_historical_data(i)\n",
        "        if (np.any(temp_history) or np.any(temp_appraisal)):                  \n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4L1w4TMk_9",
        "colab_type": "text"
      },
      "source": [
        "# Scraping Property Summary Pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1VZEeIMMBRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "while True:\n",
        "  print(i)\n",
        "  _property_summary = get_property_summary(i)\n",
        "  # print(_property_summary['Map & Parcel'].values)\n",
        "  # print(_property_summary)\n",
        "\n",
        "  if _property_summary['Map & Parcel'].values == '':\n",
        "    try:\n",
        "        end  = terminate(i, 10)\n",
        "        print ('Checking to terminate')  \n",
        "        if end == True:\n",
        "            print('Scraping Ends. Last propert # scraped is', i-1)\n",
        "            print (_property_summary)\n",
        "            summary = pd.read_csv('property_summary.csv')\n",
        "            summary = pd.concat((summary, property_summary))\n",
        "            summary.to_csv('property_summary.csv', index_label=False)\n",
        "            break\n",
        "    except:\n",
        "        print('Exception occured in checking termination for index', i)      \n",
        "        break\n",
        "\n",
        "  if i % 1000 == 1:\n",
        "    print('Creating new dataframe')\n",
        "    if np.any(_property_summary):\n",
        "        property_summary = _property_summary\n",
        "  else:\n",
        "    if np.any(_property_summary):\n",
        "        property_summary = pd.concat((property_summary, _property_summary))\n",
        "    if i%1000 == 0:\n",
        "      if 'property_summary.csv' in os.listdir('./'): #directory to be changed\n",
        "        read = pd.read_csv('property_summary.csv')    \n",
        "        read = pd.concat((read, property_summary))\n",
        "        # print('Saving the data so far to csv')\n",
        "        read.to_csv('property_summary.csv', index_label=False)\n",
        "      else:\n",
        "        if np.any(_property_summary):\n",
        "            property_summary = pd.concat((property_summary, _property_summary))\n",
        "        # print('Saving Initial file')\n",
        "        property_summary.to_csv('property_summary.csv', index_label=False)\n",
        "  i = i+1\n",
        "\n",
        "\n",
        "time.sleep(5)\n",
        "import boto3\n",
        "\n",
        "ACCESS_KEY = ''\n",
        "SECRET_KEY = ''\n",
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, \n",
        "\t\t\t\t\t\taws_secret_access_key=SECRET_KEY)\n",
        "# s3.create_bucket(Bucket=\"scraper-bucket\",\n",
        "#                  CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
        "files = [\"Property Summary.csv\"]\n",
        "\n",
        "for i in files:\n",
        "    # upload files to the bucket\n",
        "    s3.upload_file(i, \"v-scraper-bucket\", i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIQ1_C1YMgSr",
        "colab_type": "text"
      },
      "source": [
        "# Scraping Building History Pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJEva5SrMMUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "while True:\n",
        "  print(i)\n",
        "  try:\n",
        "    _sales_history, _previous_appraisals = get_historical_data(i)\n",
        "    # print(_sales_history['Map & Parcel'].values)\n",
        "    # print(_sales_history)\n",
        "\n",
        "    if (not np.any(_sales_history['Map & Parcel'].values)) and (not np.any(_previous_appraisals['Map & Parcel'].values)):\n",
        "      # try:\n",
        "          end  = terminate(i, 10)\n",
        "          print ('Checking to terminate')  \n",
        "          if end == True:\n",
        "              print('Scraping Ends. Last propert # scraped is', i-1)\n",
        "              print (_sales_history, _previous_appraisals)\n",
        "\n",
        "              sales = pd.read_csv('Sales History.csv')\n",
        "              sales = pd.concat((sales, sales_history))\n",
        "              sales.to_csv('Sales History.csv')\n",
        "              apprasals = pd.read_csv('Previous Appraisals.csv')\n",
        "              apprasals = pd.concat((apprasals, previous_appraisals))\n",
        "              apprasals.to_csv('Previous Appraisals.csv', index_label=False)\n",
        "              break\n",
        "      # except:\n",
        "          print('Exception occured in checking termination for index', i)      \n",
        "          break\n",
        "    if i % 1000 == 1:\n",
        "      print('Creating new dataframe')\n",
        "      if np.any(_sales_history):\n",
        "        sales_history, previous_appraisals = _sales_history, _previous_appraisals\n",
        "    else:\n",
        "      if np.any(_sales_history):\n",
        "        sales_history = pd.concat((sales_history, _sales_history))\n",
        "      \n",
        "      if np.any(_previous_appraisals):\n",
        "        previous_appraisals = pd.concat((previous_appraisals, _previous_appraisals))\n",
        "      \n",
        "      if i%1000 == 0:\n",
        "        # if the sales history.csv already exists, then append the data to this file\n",
        "        if 'Sales History.csv' in os.listdir('./'): \n",
        "          read = pd.read_csv('Sales History.csv')    \n",
        "          read = pd.concat((read, sales_history))\n",
        "          print('Saving the data so far to csv')\n",
        "          read.to_csv('Sales History.csv', index_label=False)\n",
        "        else: # if the sales history.csv do not exists, then create a file\n",
        "          print('Saving Initial file')\n",
        "          sales_history.to_csv('Sales History.csv', index_label=False)\n",
        "\n",
        "        # if the Previous Appraisals.csv already exists, then append the data to this file\n",
        "        if 'Previous Appraisals.csv' in os.listdir('./'): #directory to be changed\n",
        "          read = pd.read_csv('Previous Appraisals.csv')    \n",
        "          read = pd.concat((read, previous_appraisals))\n",
        "          print('Saving the data so far to csv')\n",
        "          read.to_csv('Previous Appraisals.csv', index_label=False)\n",
        "        else: # if the Previous Appraisals.csv doesn't exists, then create a new file\n",
        "          print('Saving Initial file')\n",
        "          previous_appraisals.to_csv('Previous Appraisals.csv', index_label=False)\n",
        "  except:\n",
        "    pass\n",
        "  i = i+1\n",
        "  time.sleep(1)\n",
        "\n",
        "\n",
        "\n",
        "import boto3\n",
        "\n",
        "ACCESS_KEY = ''\n",
        "SECRET_KEY = ''\n",
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, \n",
        "\t\t\t\t\t\taws_secret_access_key=SECRET_KEY)\n",
        "# s3.create_bucket(Bucket=\"scraper-bucket\",\n",
        "#                  CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
        "files = [\"Sales History.csv\",\n",
        "         \"Previous Appraisals.csv\"]\n",
        "\n",
        "for i in files:\n",
        "    # upload files to the bucket\n",
        "    s3.upload_file(i, \"v-scraper-bucket\", i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERQjg2PkMX-w",
        "colab_type": "text"
      },
      "source": [
        "# Scrape Building Improvement Pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BxBwLUHMP2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "while True:\n",
        "  print(i)\n",
        "  try:\n",
        "    _building_improvement = get_building_improvement(i)\n",
        "    # print(_building_improvement['Map & Parcel'].values)\n",
        "    # print(_building_improvement)\n",
        "\n",
        "    if not np.any(_building_improvement['Map & Parcel'].values):\n",
        "      try:\n",
        "        end  = terminate(i, 10)\n",
        "        print ('Checking to terminate')  \n",
        "        if end == True:\n",
        "            print('Scraping Ends. Last propert # scraped is', i)\n",
        "            print (_building_improvement)\n",
        "            improvement = pd.read_csv('Building Improvement.csv')\n",
        "            improvement = pd.concat((improvement, building_improvement))\n",
        "            improvement.to_csv('Building Improvement.csv', index_label=False)\n",
        "            break\n",
        "      except:\n",
        "        print('Exception occured in checking termination for index', i)      \n",
        "        break\n",
        "\n",
        "    if i % 1000 == 1:\n",
        "      # print('Creating new dataframe')\n",
        "      if np.any(_building_improvement):\n",
        "        building_improvement = _building_improvement\n",
        "    else:\n",
        "      if np.any(_building_improvement):\n",
        "        building_improvement = pd.concat((building_improvement, _building_improvement))\n",
        "      \n",
        "      if i%1000 == 0:\n",
        "        # if the building improvement.csv already exists, then append the data to this file\n",
        "        if 'Building Improvement.csv' in os.listdir('./'): \n",
        "          read = pd.read_csv('Building Improvement.csv')    \n",
        "          read = pd.concat((read, building_improvement))\n",
        "          # print('Saving the data so far to csv')\n",
        "          read.to_csv('Building Improvement.csv', index_label=False)\n",
        "        else: # if the sales history.csv do not exists, then create a file\n",
        "          # print('Saving Initial file')\n",
        "          building_improvement.to_csv('Building Improvement.csv', index_label=False)\n",
        "  except:\n",
        "    pass\n",
        "  i = i+1\n",
        "  time.sleep(1)\n",
        "\n",
        "#sending files to s3  \n",
        "import boto3\n",
        "\n",
        "ACCESS_KEY = ''\n",
        "SECRET_KEY = ''\n",
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, \n",
        "\t\t\t\t\t\taws_secret_access_key=SECRET_KEY)\n",
        "# s3.create_bucket(Bucket=\"scraper-bucket\",\n",
        "#                  CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
        "files = [\"Building Improvement.csv\"]\n",
        "\n",
        "for i in files:\n",
        "    # upload files to the bucket\n",
        "    s3.upload_file(i, \"v-scraper-bucket\", i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYT34u-HMPvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UrYCKy4IN4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QxVuNTxIN1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb4mQkRfINyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjNJs4C2mwbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdpaKGIQk3CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}